{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "823e07ae",
   "metadata": {},
   "source": [
    "# Domain Spinning Auto-Checker\n",
    "\n",
    "Once you've Saved the Configs for a list of domains, you can enter said list here, and this code will automatically do the following every ~30 seconds:\n",
    "\n",
    "- Check each domain in your list\n",
    "- Let you know if they're ready, and whether or not they've shunted correctly\n",
    "- Let you know how many are left, ocassionally in cowboy parlance and rarely in pirate parlance\n",
    "\n",
    "And if nothing happens, every once in a while it'll tell you how many minutes it's been running, and how many domains are left.\n",
    "\n",
    "To operate this, just toss the domains you'd like to check in the first cell, then press >Run twice. That's it!\n",
    "\n",
    "Finally, it works *almost* perfectly, but you'll want to double check at least the remon domains, and it'll stop running after three hours - at that point, you should probably check and respin any undone domains anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3994d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# internet\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "\n",
    "# system and time\n",
    "import time\n",
    "import os\n",
    "\n",
    "# partial and pandas\n",
    "from functools import partial\n",
    "\n",
    "'''\n",
    "-----------------------\n",
    "Setup - Enter URLs here\n",
    "-----------------------\n",
    "'''\n",
    "\n",
    "urls = '''fake_url1.fyi\n",
    "fake_url2.fyi\n",
    "fake_url3.zone\n",
    "'''.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea69168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  --Goood to go--\n",
      "a-great-us-vinyl-flooring.fyi\n",
      "  --Remon needed--\n",
      "fetch-an-us-hardwood-flooring.fyi\n",
      "-\n",
      "Run time: 0.6 minutes - doms left: 1\n",
      "-\n",
      "Run time: 1.1 minutes - doms left: 1\n",
      "-\n",
      "Run time: 1.6 minutes - doms left: 1\n",
      "-\n",
      "Run time: 2.1 minutes - doms left: 1\n",
      "-\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w4/d9gf0jkj61qd594l75lf8s9w0000gn/T/ipykernel_90163/3691335232.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Run time: {run_time:0.1f} minutes - doms left: {num_left}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# once time limit exceeded, print remainder:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hour_cap = 3  # the number of hours for which you'd like this to run\n",
    "\n",
    "long_time_counter = 10  # minutes after which you'd like fewer notifications\n",
    "\n",
    "# have to set up the following outside the loop\n",
    "\n",
    "start_time = time.perf_counter()  # reference time\n",
    "time.sleep(.1)\n",
    "run_time = (time.perf_counter() - start_time)/60  # for while() logic purposes\n",
    "\n",
    "# domain bins upon completion or interruption - these get passed to Selenium\n",
    "google_url_storage, yahoo_url_storage, fail_url_storage = [], [], []\n",
    "# ^ probably a ~slightly~ cleaner way to do that, particularly on the RHS - np?\n",
    "\n",
    "# misc\n",
    "num_left = len(urls)\n",
    "long_time_bool = False\n",
    "print()\n",
    "\n",
    "'''\n",
    "---------\n",
    "Functions\n",
    "---------\n",
    "'''\n",
    "\n",
    "\n",
    "def fun_roll(num_left):\n",
    "    '''\n",
    "    If you want the comp to say more than \"there are n left\"\n",
    "    Feel free to add an alien version, a robot version; have at it\n",
    "    '''\n",
    "\n",
    "    die_roll = np.random.uniform(0, 1, 1)[0]  # random on (0,1)\n",
    "\n",
    "    if die_roll < .88:\n",
    "        os.system(f\"say '{num_left} left'\")\n",
    "    elif die_roll >= .88 and die_roll < .98:\n",
    "        # 10% roll\n",
    "        os.system(f\"say 'I reckon there are {num_left} left, partner'\")\n",
    "    else:\n",
    "        # 2% roll\n",
    "        os.system(f\"say 'Yar, there be {num_left} left, me hearty'\")\n",
    "\n",
    "\n",
    "def url_checker(urls):\n",
    "    '''\n",
    "    The gist:\n",
    "        - first, try to load URLs, and if they load to something with 'script'\n",
    "        fields and either yahoo or google links, push the url into the relevant\n",
    "        buckets\n",
    "        - second, since correctly shunting google domains can apparently,\n",
    "        temporarily, show yahoo links, try any yahoo domains again after 30s\n",
    "        - third, print the results\n",
    "\n",
    "    Note: this may artificially take 30s to run - no need to worry if so\n",
    "    '''\n",
    "\n",
    "    google_urls, yahoo_urls, fail_urls = [], [], []\n",
    "\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) '\n",
    "               + 'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "               + 'Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        # try to load the page\n",
    "        try:\n",
    "            page = requests.get(f'http://{url}', headers=headers)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        except:\n",
    "            fail_urls.append(url)\n",
    "\n",
    "        # then, does it have google links?\n",
    "        try:\n",
    "            if any(['google' in scrpt.string for scrpt in\n",
    "                    soup.body.findAll('script')]):\n",
    "                google_urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # how about yahoo?\n",
    "        try:\n",
    "            if any(['.y.' in link['href'] for link in soup.findAll('a')]):\n",
    "                yahoo_urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # robust to false remon - could be an option\n",
    "    # so far, experimental\n",
    "    # basically, if we get false Yahoos, wait 30s, try again, and update\n",
    "\n",
    "    if yahoo_urls:\n",
    "\n",
    "        time.sleep(30)  # wait - this may fix the Yahoo remon bug\n",
    "        yahoo_actual_urls = yahoo_urls.copy()\n",
    "\n",
    "        for url in yahoo_urls:\n",
    "            page = requests.get(f'http://{url}', headers=headers)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                if any(['google' in scrpt.string for scrpt in\n",
    "                        soup.body.findAll('script')]):\n",
    "                    google_urls.append(url)\n",
    "                    yahoo_actual_urls.drop(url)\n",
    "            except:\n",
    "                yahoo_url_storage.append(url)\n",
    "\n",
    "        yahoo_urls = yahoo_actual_urls.copy()\n",
    "\n",
    "    return google_urls, yahoo_urls, fail_urls\n",
    "\n",
    "\n",
    "def url_viewer(urls):\n",
    "    '''\n",
    "    Opens all domains one by one in new tabs to verify proper KW honor rate\n",
    "    '''\n",
    "\n",
    "    # start driver\n",
    "    driver = webdriver.Chrome(executable_path='drivers/chromedriver')\n",
    "\n",
    "    for url in urls:\n",
    "        driver.get(f\"https://{url}\")\n",
    "        time.sleep(3)\n",
    "        driver.switch_to.new_window('tab')\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    print(), print('URLs tested:'), print(), print(*urls, sep='\\n')\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "'''\n",
    "-----------\n",
    "Script Body\n",
    "-----------\n",
    "'''\n",
    "\n",
    "while(int(run_time / 60) < hour_cap):\n",
    "    '''\n",
    "    The gist:\n",
    "        - first, try to load URLs, and if they load to something with 'script'\n",
    "        fields and either yahoo or google links, push the url into the relevant\n",
    "        buckets\n",
    "        - second, since correctly shunting google domains can apparently,\n",
    "        temporarily, show yahoo links, try any yahoo domains again after 30s\n",
    "        - third, print the results\n",
    "        - fourth, print the run time - all of the fluff is just to make it\n",
    "        look nicer and minimize spamming the console\n",
    "        - is it running for more than hour_cap hours? If so, bail\n",
    "    '''\n",
    "\n",
    "    keep_urls = urls.copy()  # this allows us to remove urls from the list\n",
    "\n",
    "    # url fishing\n",
    "\n",
    "    google_urls, yahoo_urls, fail_urls = url_checker(urls)\n",
    "\n",
    "    # url update printing\n",
    "    # note: long term, I should look into alerting vs just printing\n",
    "\n",
    "    if (google_urls + yahoo_urls + fail_urls):\n",
    "\n",
    "        # remove urls from running urls list\n",
    "        for url in google_urls + yahoo_urls + fail_urls:\n",
    "            keep_urls.remove(url)\n",
    "\n",
    "        # merge the new list\n",
    "        urls = keep_urls.copy()\n",
    "\n",
    "        # alerts, storage updates\n",
    "        if google_urls:\n",
    "            print('  --Goood to go--')\n",
    "            print(*google_urls, sep='\\n')\n",
    "            google_url_storage += google_urls\n",
    "        if yahoo_urls:\n",
    "            print('  --Remon needed--')\n",
    "            print(*yahoo_urls, sep='\\n')\n",
    "            yahoo_url_storage += yahoo_urls\n",
    "        if fail_urls:\n",
    "            print('  --Failed to load--')\n",
    "            print(*fail_urls, sep='\\n')\n",
    "            fail_url_storage += fail_urls\n",
    "        print('-')\n",
    "\n",
    "        # how many left? Or, done\n",
    "        num_left = len(keep_urls)\n",
    "\n",
    "        if num_left > 0:\n",
    "            fun_roll(num_left)  # or, lame, print \"x left\"\n",
    "        else:\n",
    "            # if we pull the last url, we're done\n",
    "            print('\\a')\n",
    "            time.sleep(.5)\n",
    "            print('\\a')  # double ping\n",
    "            os.system(\"say 'done'\")\n",
    "            break\n",
    "\n",
    "    # run time handling\n",
    "\n",
    "    run_time = (time.perf_counter() - start_time)/60\n",
    "\n",
    "    # if run time is over an hour (or whatever), fewer updates\n",
    "\n",
    "    if run_time > long_time_counter:\n",
    "\n",
    "        long_time_bool = True  # only need this once, technically\n",
    "        hours, minutes = int(run_time / 60), int(run_time % 60)\n",
    "        # int takes floor, which we want; 1.43 hours -> 1 hour\n",
    "\n",
    "        if hours > 0:\n",
    "            print(f'Run time: {hours} hours, {minutes} minutes - doms left: {num_left}')\n",
    "            long_time_counter += 5  # tell me every 5 minutes that nothing happened\n",
    "        else:\n",
    "            print(f'Run time: {minutes} minutes - doms left: {num_left}')\n",
    "            long_time_counter += 2  # tell me every 2 minutes\n",
    "        print('-')\n",
    "        time.sleep(60)\n",
    "\n",
    "    elif long_time_bool is False:\n",
    "        print(f'Run time: {run_time:0.1f} minutes - doms left: {num_left}')\n",
    "        print('-')\n",
    "        time.sleep(30)\n",
    "\n",
    "# once time limit exceeded, print remainder:\n",
    "\n",
    "print('Summary:')\n",
    "if google_url_storage:\n",
    "    print(), print('  --Good to go--')\n",
    "    print(*google_url_storage, sep='\\n')\n",
    "if yahoo_url_storage:\n",
    "    print(), print('  --Remon needed--')\n",
    "    print(*yahoo_url_storage, sep='\\n')\n",
    "if fail_url_storage:\n",
    "    print(), print('  --Failed to load--')\n",
    "    print(*fail_url_storage, sep='\\n')\n",
    "if urls:\n",
    "    print(), print('  --Never monetized--')\n",
    "    print(*urls, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7171340a",
   "metadata": {},
   "source": [
    "### Nice\n",
    "\n",
    "Good for you for checking down here! Did you interrupt the script, but want a summary of what happened while it ran? Here you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71376b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "\n",
      "  --Good to go--\n",
      "a-great-us-vinyl-flooring.fyi\n",
      "\n",
      "  --Remon needed--\n",
      "fetch-an-us-hardwood-flooring.fyi\n",
      "\n",
      "  --Never monetized--\n",
      "have-us-psoriatic-arthritis.fyi\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "if google_url_storage:\n",
    "    print(), print('  --Good to go--')\n",
    "    print(*google_url_storage, sep='\\n')\n",
    "if yahoo_url_storage:\n",
    "    print(), print('  --Remon needed--')\n",
    "    print(*yahoo_url_storage, sep='\\n')\n",
    "if fail_url_storage:\n",
    "    print(), print('  --Failed to load--')\n",
    "    print(*fail_url_storage, sep='\\n')\n",
    "if urls:\n",
    "    print(), print('  --Never monetized--')\n",
    "    print(*urls, sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
